{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETDS Explained\n",
    "\n",
    "\n",
    "Paper: Equivalent Transformation and Dual Stream Network Construction for Mobile Image Super-Resolution [[Link]](https://openaccess.thecvf.com/content/CVPR2023/papers/Chao_Equivalent_Transformation_and_Dual_Stream_Network_Construction_for_Mobile_Image_CVPR_2023_paper.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Horizontal Concatenation - Eqn.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1, 32, 64, 64)\n",
    "y = torch.randn(1, 32, 64, 64)\n",
    "\n",
    "w1 = torch.randn(64, 32, 3, 3)\n",
    "w2 = torch.randn(64, 32, 3, 3)\n",
    "\n",
    "res_1 = F.conv2d(x, w1, padding=1) + F.conv2d(y, w2, padding=1)\n",
    "\n",
    "# This is horizontal concatenation\n",
    "res_2 = F.conv2d(\n",
    "    torch.cat([x, y], dim=1), torch.cat([w1, w2], dim=1), padding=1\n",
    ")\n",
    "\n",
    "torch.allclose(res_1, res_2, atol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Vertical Concatenation - Eqn.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1, 32, 64, 64)\n",
    "\n",
    "w1 = torch.randn(32, 32, 3, 3)\n",
    "w2 = torch.randn(32, 32, 3, 3)\n",
    "\n",
    "res_1 = torch.cat([F.conv2d(x, w1, padding=1), F.conv2d(x, w2, padding=1)], dim=1)\n",
    "\n",
    "# This is vertical concatenation\n",
    "res_2 = F.conv2d(x, torch.cat([w1, w2], dim=0), padding=1)\n",
    "\n",
    "torch.allclose(res_1, res_2, atol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ET for Repeat Operation - Eqn.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1, 32, 64, 64)\n",
    "\n",
    "I = torch.eye(32).view(32, 32, 1, 1)\n",
    "\n",
    "n_concat = 4\n",
    "x_concat_1 = torch.cat([x] * n_concat, dim=1)\n",
    "\n",
    "# Vertical concatenation\n",
    "x_concat_2 = F.conv2d(x, torch.cat([I] * n_concat, dim=0), padding=0)\n",
    "\n",
    "torch.allclose(x_concat_1, x_concat_2, atol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ET for Add Operation - Eqn.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1, 32, 64, 64)\n",
    "y = torch.randn(1, 32, 64, 64)\n",
    "\n",
    "I = torch.eye(32).view(32, 32, 1, 1)\n",
    "\n",
    "sum_1 = x + y\n",
    "\n",
    "# Horizontal concatenation\n",
    "sum_2 = F.conv2d(\n",
    "    torch.cat([x, y], dim=1), torch.cat([I, I], dim=1), padding=0\n",
    ")\n",
    "\n",
    "torch.allclose(sum_1, sum_2, atol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ET for Concat Operation - Eqn.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1, 32, 64, 64)\n",
    "y = torch.randn(1, 32, 64, 64)\n",
    "\n",
    "w1 = torch.randn(32, 32, 3, 3)\n",
    "b1 = torch.randn(32)\n",
    "w2 = torch.randn(32, 32, 3, 3)\n",
    "b2 = torch.randn(32)\n",
    "\n",
    "O = torch.zeros(32, 32, 3, 3)\n",
    "\n",
    "cat_1= torch.cat([F.conv2d(x, w1, padding=1) + b1.view(1, -1, 1, 1), F.conv2d(y, w2, padding=1) + b2.view(1, -1, 1, 1)], dim=1)\n",
    "\n",
    "# Horizontal concatenation + vertical concatenation\n",
    "cat_2 = F.conv2d(\n",
    "    torch.cat([x, y], dim=1), torch.cat([torch.cat([w1, O], dim=1), torch.cat([O, w2], dim=1)], dim=0), padding=1\n",
    ") + torch.cat([b1, b2], dim=0).view(1, -1, 1, 1)\n",
    "\n",
    "torch.allclose(cat_1, cat_2, atol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ET for Concat Operation(Where one residual has no convolution) - Eqn.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1, 32, 64, 64)\n",
    "y = torch.randn(1, 32, 64, 64)\n",
    "\n",
    "w1 = torch.randn(32, 32, 3, 3)\n",
    "b1 = torch.randn(32)\n",
    "\n",
    "I = torch.eye(32).view(32, 32, 1, 1)\n",
    "I = F.pad(I, (1, 1, 1, 1))  # To 3x3\n",
    "O = torch.zeros(32, 32, 3, 3)\n",
    "\n",
    "cat_1 = torch.cat([F.conv2d(x, w1, padding=1) + b1.view(1, -1, 1, 1), y], dim=1)\n",
    "\n",
    "# Horizontal concatenation + vertical concatenation\n",
    "cat_2 = F.conv2d(\n",
    "    torch.cat([x, y], dim=1), torch.cat([torch.cat([w1, O], dim=1), torch.cat([O, I], dim=1)], dim=0), padding=1\n",
    ") + torch.cat([b1, torch.zeros_like(b1)], dim=0).view(1, -1, 1, 1)\n",
    "\n",
    "torch.allclose(cat_1, cat_2, atol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ET for Concat Operation(Where one residual has no convolution and x equals y) - Eqn.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1, 32, 64, 64)\n",
    "y = x.clone()\n",
    "\n",
    "w1 = torch.randn(32, 32, 3, 3)\n",
    "b1 = torch.randn(32)\n",
    "\n",
    "I = torch.eye(32).view(32, 32, 1, 1)\n",
    "I = F.pad(I, (1, 1, 1, 1))  # To 3x3\n",
    "\n",
    "cat_1 = torch.cat([F.conv2d(x, w1, padding=1) + b1.view(1, -1, 1, 1), y], dim=1)\n",
    "\n",
    "# Vertical concatenation\n",
    "cat_2 = F.conv2d(\n",
    "    x, torch.cat([w1, I], dim=0), padding=1\n",
    ") + torch.cat([b1, torch.zeros_like(b1)], dim=0).view(1, -1, 1, 1)\n",
    "\n",
    "torch.allclose(cat_1, cat_2, atol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ET for Clip Operation - Eqn.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1, 32, 64, 64) * 255\n",
    "\n",
    "I = torch.eye(32).view(32, 32, 1, 1)\n",
    "\n",
    "clip_1 = torch.clip(x, 0, 255)\n",
    "\n",
    "# clip(x) equals ReLU(-ReLU(-x + 255) + 255)\n",
    "\n",
    "y = F.relu(F.conv2d(x, -I, padding=0) + 255)\n",
    "clip_2 = F.relu(F.conv2d(y, -I, padding=0) + 255)\n",
    "\n",
    "torch.allclose(clip_1, clip_2, atol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Convert PlainSR using ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "def conv_1x1(x, w):\n",
    "    return F.conv2d(x, w)\n",
    "\n",
    "def conv_3x3(x, w):\n",
    "    return F.conv2d(x, w, padding=1)\n",
    "\n",
    "def relu(x):  # Actually, PlainSR used PReLU but for convenience we use ReLU\n",
    "    return F.relu(x)\n",
    "\n",
    "\n",
    "# This is M3C32 Example. For convenience, bias is omitted.\n",
    "def PlainSR(x, w_proj, w_1, w_2, w_3, w_out, scale=4):\n",
    "    x_skip = x.clone()\n",
    "\n",
    "    # body\n",
    "    x = relu(conv_3x3(x, w_proj))\n",
    "    x = relu(conv_3x3(x, w_1))\n",
    "    x = relu(conv_3x3(x, w_2))\n",
    "    x = relu(conv_3x3(x, w_3))\n",
    "    x = conv_3x3(x, w_out)\n",
    "    \n",
    "    # skip connection\n",
    "    x = x + x_skip.repeat(1, scale ** 2, 1, 1)  # This equals F.pixel_shuffle(x) + nn.UpSample(scale_factor=scale, mode='nearest')(x_skip)\n",
    "    \n",
    "    # clip\n",
    "    x = torch.clip(x, 0, 255)\n",
    "    \n",
    "    # upsample\n",
    "    x = F.pixel_shuffle(x, scale)\n",
    "    return x\n",
    "\n",
    "\n",
    "x = torch.FloatTensor(1, 3, 64, 64).uniform_(0, 1)\n",
    "w_proj = torch.randn(32, 3, 3, 3)\n",
    "w_1 = torch.randn(32, 32, 3, 3)\n",
    "w_2 = torch.randn(32, 32, 3, 3)\n",
    "w_3 = torch.randn(32, 32, 3, 3)\n",
    "w_out = torch.randn(3 * 4 ** 2, 32, 3, 3)  # Colors * Scale ** 2, inp_planes, kernel_size, kernel_size\n",
    "\n",
    "res_1 = PlainSR(x, w_proj, w_1, w_2, w_3, w_out)\n",
    "\n",
    "print(res_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Convert Repeat Operation into Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def PlainSR_repeat_to_conv(x, w_proj, w_1, w_2, w_3, w_out, scale=4):\n",
    "    x_skip = x.clone()\n",
    "    I_input = torch.cat(\n",
    "        [torch.eye(x.shape[1]).view(x.shape[1], x.shape[1], 1, 1)] * scale ** 2, dim=0\n",
    "    )\n",
    "\n",
    "    # body\n",
    "    x = relu(conv_3x3(x, w_proj))\n",
    "    x = relu(conv_3x3(x, w_1))\n",
    "    x = relu(conv_3x3(x, w_2))\n",
    "    x = relu(conv_3x3(x, w_3))\n",
    "    x = conv_3x3(x, w_out)\n",
    "    \n",
    "    # skip connection\n",
    "    x_skip = conv_1x1(x_skip, I_input) # <<<<<<<<<<<<<<<<<<<<<\n",
    "    x = x + x_skip\n",
    "    \n",
    "    # clip\n",
    "    x = torch.clip(x, 0, 255)\n",
    "    \n",
    "    # upsample\n",
    "    x = F.pixel_shuffle(x, scale)\n",
    "    return x\n",
    "\n",
    "res_2 = PlainSR_repeat_to_conv(x, w_proj, w_1, w_2, w_3, w_out)\n",
    "\n",
    "torch.allclose(res_1, res_2, atol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Convert Add Operation into Concat and Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def PlainSR_add_to_conv(x, w_proj, w_1, w_2, w_3, w_out, scale=4):\n",
    "    x_skip = x.clone()\n",
    "    I_input = torch.cat(\n",
    "        [torch.eye(x.shape[1]).view(x.shape[1], x.shape[1], 1, 1)] * scale ** 2, dim=0\n",
    "    )\n",
    "    I_out = torch.eye(w_out.shape[0]).view(w_out.shape[0], w_out.shape[0], 1, 1).repeat(1, 2, 1, 1) # <<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "    # body\n",
    "    x = relu(conv_3x3(x, w_proj))\n",
    "    x = relu(conv_3x3(x, w_1))\n",
    "    x = relu(conv_3x3(x, w_2))\n",
    "    x = relu(conv_3x3(x, w_3))\n",
    "    x = conv_3x3(x, w_out)\n",
    "    \n",
    "    # skip connection\n",
    "    x_skip = conv_1x1(x_skip, I_input) \n",
    "    x_cat = torch.cat([x, x_skip], dim=1)  # <<<<<<<<<<<<<<<<<<<<<\n",
    "    x = conv_1x1(x_cat, I_out)   # <<<<<<<<<<<<<<<<<<<<<\n",
    "    \n",
    "    # clip\n",
    "    x = torch.clip(x, 0, 255)\n",
    "    \n",
    "    # upsample\n",
    "    x = F.pixel_shuffle(x, scale)\n",
    "    return x\n",
    "\n",
    "res_3 = PlainSR_add_to_conv(x, w_proj, w_1, w_2, w_3, w_out)\n",
    "\n",
    "torch.allclose(res_1, res_3, atol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Convert Concat Operation into Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def PlainSR_concat_to_conv(x, w_proj, w_1, w_2, w_3, w_out, scale=4):\n",
    "    x_skip = x.clone()\n",
    "    I_input = torch.cat(\n",
    "        [torch.eye(x.shape[1]).view(x.shape[1], x.shape[1], 1, 1)] * scale ** 2, dim=0\n",
    "    )\n",
    "    I_input = F.pad(I_input, (1, 1, 1, 1))  # To 3x3 <<<<<<<<<<<<<<<<<<<<<\n",
    "    I_out = torch.eye(w_out.shape[0]).view(w_out.shape[0], w_out.shape[0], 1, 1).repeat(1, 2, 1, 1)\n",
    "\n",
    "    w_out_et = torch.cat(\n",
    "        [torch.cat([w_out, torch.zeros_like(I_input)], dim=1), torch.cat([torch.zeros_like(w_out), I_input], dim=1)], dim=0\n",
    "    )\n",
    "\n",
    "    # body\n",
    "    x = relu(conv_3x3(x, w_proj))\n",
    "    x = relu(conv_3x3(x, w_1))\n",
    "    x = relu(conv_3x3(x, w_2))\n",
    "    x = relu(conv_3x3(x, w_3))\n",
    "    # x = conv_3x3(x, w_out)\n",
    "    x_cat_bef = torch.cat([x, x_skip], dim=1) # <<<<<<<<<<<<<<<<<<<<<\n",
    "    x_cat = conv_3x3(x_cat_bef, w_out_et) # <<<<<<<<<<<<<<<<<<<<<\n",
    "    \n",
    "    # skip connection\n",
    "    x = conv_1x1(x_cat, I_out)   # <<<<<<<<<<<<<<<<<<<<<\n",
    "    \n",
    "    # clip\n",
    "    x = torch.clip(x, 0, 255)\n",
    "    \n",
    "    # upsample\n",
    "    x = F.pixel_shuffle(x, scale)\n",
    "    \n",
    "    return x\n",
    "\n",
    "res_4 = PlainSR_concat_to_conv(x, w_proj, w_1, w_2, w_3, w_out)\n",
    "\n",
    "torch.allclose(res_1, res_4, atol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Structural re-parameterize Concat Conv and Skip Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def merge_sequential_conv(w1, w2):\n",
    "    w1 = w1.permute(2, 3, 1, 0)\n",
    "    w2 = w2.permute(2, 3, 1, 0)\n",
    "    k = w1 @ w2\n",
    "    k = k.permute(3, 2, 0, 1)\n",
    "    return k\n",
    "\n",
    "def PlainSR_reparam_concat_and_skip(x, w_proj, w_1, w_2, w_3, w_out, scale=4):\n",
    "    x_skip = x.clone()\n",
    "    I_input = torch.cat(\n",
    "        [torch.eye(x.shape[1]).view(x.shape[1], x.shape[1], 1, 1)] * scale ** 2, dim=0\n",
    "    )\n",
    "    I_input = F.pad(I_input, (1, 1, 1, 1))\n",
    "    I_out = torch.eye(w_out.shape[0]).view(w_out.shape[0], w_out.shape[0], 1, 1).repeat(1, 2, 1, 1)\n",
    "\n",
    "    w_out_et = torch.cat(\n",
    "        [torch.cat([w_out, torch.zeros_like(I_input)], dim=1), torch.cat([torch.zeros_like(w_out), I_input], dim=1)], dim=0\n",
    "    )\n",
    "    \n",
    "    w_concat_and_skip_et = merge_sequential_conv(w_out_et, I_out) # <<<<<<<<<<<<<<<<<<<<<\n",
    "    \n",
    "    # body\n",
    "    x = relu(conv_3x3(x, w_proj))\n",
    "    x = relu(conv_3x3(x, w_1))\n",
    "    x = relu(conv_3x3(x, w_2))\n",
    "    x = relu(conv_3x3(x, w_3))\n",
    "    x_cat_bef = torch.cat([x, x_skip], dim=1) \n",
    "    \n",
    "    # concat + skip\n",
    "    x = conv_3x3(x_cat_bef, w_concat_and_skip_et) # <<<<<<<<<<<<<<<<<<<<<\n",
    "    # x_cat = conv_3x3(x_cat_bef, w_out_et)\n",
    "    \n",
    "    # # skip connection\n",
    "    # x = conv_1x1(x_cat, I_out)\n",
    "    \n",
    "    # clip\n",
    "    x = torch.clip(x, 0, 255)\n",
    "    \n",
    "    # upsample\n",
    "    x = F.pixel_shuffle(x, scale)\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "res_5 = PlainSR_reparam_concat_and_skip(x, w_proj, w_1, w_2, w_3, w_out)\n",
    "\n",
    "torch.allclose(res_1, res_5, atol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Convert Concat Operation into Convolution(Where one residual has no convolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def PlainSR_concat_noconvres_to_conv(x, w_proj, w_1, w_2, w_3, w_out, scale=4):\n",
    "    x_skip = x.clone()\n",
    "    I = torch.eye(x.shape[1]).view(x.shape[1], x.shape[1], 1, 1)\n",
    "    I = F.pad(I, (1, 1, 1, 1))  # To 3x3 <<<<<<<<<<<<<<<<<<<<<\n",
    "    I_input = torch.cat(\n",
    "        [torch.eye(x.shape[1]).view(x.shape[1], x.shape[1], 1, 1)] * scale ** 2, dim=0\n",
    "    )\n",
    "    I_input = F.pad(I_input, (1, 1, 1, 1))\n",
    "    I_out = torch.eye(w_out.shape[0]).view(w_out.shape[0], w_out.shape[0], 1, 1).repeat(1, 2, 1, 1)\n",
    "    \n",
    "    w_3_et = torch.cat(\n",
    "        [torch.cat([w_3, torch.zeros((w_3.shape[0], I.shape[1], 3, 3))], dim=1), torch.cat([torch.zeros((I.shape[0], w_3.shape[1], 3, 3)), I], dim=1)], dim=0\n",
    "    )\n",
    "    w_out_et = torch.cat(\n",
    "        [torch.cat([w_out, torch.zeros_like(I_input)], dim=1), torch.cat([torch.zeros_like(w_out), I_input], dim=1)], dim=0\n",
    "    )\n",
    "    \n",
    "    w_concat_and_skip_et = merge_sequential_conv(w_out_et, I_out)\n",
    "    \n",
    "    # body\n",
    "    x = relu(conv_3x3(x, w_proj))\n",
    "    x = relu(conv_3x3(x, w_1))\n",
    "    x = relu(conv_3x3(x, w_2))\n",
    "    x_cat_bef_3 = torch.cat([x, x_skip], dim=1)\n",
    "    x_cat_bef = relu(conv_3x3(x_cat_bef_3, w_3_et)) # <<<<<<<<<<<<<<<<<<<<<\n",
    "    # x = relu(conv_3x3(x, w_3))\n",
    "    # x_cat_bef = torch.cat([x, x_skip], dim=1) \n",
    "    \n",
    "    # concat + skip\n",
    "    x = conv_3x3(x_cat_bef, w_concat_and_skip_et)\n",
    "    \n",
    "    # clip\n",
    "    x = torch.clip(x, 0, 255)\n",
    "    \n",
    "    # upsample\n",
    "    x = F.pixel_shuffle(x, scale)\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "res_6 = PlainSR_concat_noconvres_to_conv(x, w_proj, w_1, w_2, w_3, w_out)\n",
    "\n",
    "torch.allclose(res_1, res_6, atol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-2. Repeat this to w1 and w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def PlainSR_concat_noconvres_to_conv_2(x, w_proj, w_1, w_2, w_3, w_out, scale=4):\n",
    "    x_skip = x.clone()\n",
    "    I = torch.eye(x.shape[1]).view(x.shape[1], x.shape[1], 1, 1)\n",
    "    I = F.pad(I, (1, 1, 1, 1))\n",
    "    I_input = torch.cat(\n",
    "        [torch.eye(x.shape[1]).view(x.shape[1], x.shape[1], 1, 1)] * scale ** 2, dim=0\n",
    "    )\n",
    "    I_input = F.pad(I_input, (1, 1, 1, 1))\n",
    "    I_out = torch.eye(w_out.shape[0]).view(w_out.shape[0], w_out.shape[0], 1, 1).repeat(1, 2, 1, 1)\n",
    "    \n",
    "    w_2_et = torch.cat(\n",
    "        [torch.cat([w_2, torch.zeros((w_2.shape[0], I.shape[1], 3, 3))], dim=1), torch.cat([torch.zeros((I.shape[0], w_2.shape[1], 3, 3)), I], dim=1)], dim=0\n",
    "    )    # <<<<<<<<<<<<<<<<<<<<<\n",
    "    w_3_et = torch.cat(\n",
    "        [torch.cat([w_3, torch.zeros((w_3.shape[0], I.shape[1], 3, 3))], dim=1), torch.cat([torch.zeros((I.shape[0], w_3.shape[1], 3, 3)), I], dim=1)], dim=0\n",
    "    )\n",
    "    w_out_et = torch.cat(\n",
    "        [torch.cat([w_out, torch.zeros_like(I_input)], dim=1), torch.cat([torch.zeros_like(w_out), I_input], dim=1)], dim=0\n",
    "    )\n",
    "    \n",
    "    w_concat_and_skip_et = merge_sequential_conv(w_out_et, I_out)\n",
    "    \n",
    "    # body\n",
    "    x = relu(conv_3x3(x, w_proj))\n",
    "    x = relu(conv_3x3(x, w_1))\n",
    "    x_cat_bef_2 = torch.cat([x, x_skip], dim=1)\n",
    "    x_cat_bef_3 = relu(conv_3x3(x_cat_bef_2, w_2_et)) # <<<<<<<<<<<<<<<<<<<<<\n",
    "    # x = relu(conv_3x3(x, w_2))\n",
    "    # x_cat_bef_3 = torch.cat([x, x_skip], dim=1)\n",
    "    x_cat_bef = relu(conv_3x3(x_cat_bef_3, w_3_et))\n",
    "    \n",
    "    # concat + skip\n",
    "    x = conv_3x3(x_cat_bef, w_concat_and_skip_et)\n",
    "    \n",
    "    # clip\n",
    "    x = torch.clip(x, 0, 255)\n",
    "    \n",
    "    # upsample\n",
    "    x = F.pixel_shuffle(x, scale)\n",
    "    \n",
    "    return x\n",
    "\n",
    "res_7 = PlainSR_concat_noconvres_to_conv_2(x, w_proj, w_1, w_2, w_3, w_out)\n",
    "\n",
    "torch.allclose(res_1, res_7, atol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def PlainSR_concat_noconvres_to_conv_3(x, w_proj, w_1, w_2, w_3, w_out, scale=4):\n",
    "    x_skip = x.clone()\n",
    "    I = torch.eye(x.shape[1]).view(x.shape[1], x.shape[1], 1, 1)\n",
    "    I = F.pad(I, (1, 1, 1, 1))\n",
    "    I_input = torch.cat(\n",
    "        [torch.eye(x.shape[1]).view(x.shape[1], x.shape[1], 1, 1)] * scale ** 2, dim=0\n",
    "    )\n",
    "    I_input = F.pad(I_input, (1, 1, 1, 1))\n",
    "    I_out = torch.eye(w_out.shape[0]).view(w_out.shape[0], w_out.shape[0], 1, 1).repeat(1, 2, 1, 1)\n",
    "    \n",
    "    w_1_et = torch.cat(\n",
    "        [torch.cat([w_1, torch.zeros((w_1.shape[0], I.shape[1], 3, 3))], dim=1), torch.cat([torch.zeros((I.shape[0], w_1.shape[1], 3, 3)), I], dim=1)], dim=0\n",
    "    )   # <<<<<<<<<<<<<<<<<<<<<\n",
    "    w_2_et = torch.cat(\n",
    "        [torch.cat([w_2, torch.zeros((w_2.shape[0], I.shape[1], 3, 3))], dim=1), torch.cat([torch.zeros((I.shape[0], w_2.shape[1], 3, 3)), I], dim=1)], dim=0\n",
    "    )  \n",
    "    w_3_et = torch.cat(\n",
    "        [torch.cat([w_3, torch.zeros((w_3.shape[0], I.shape[1], 3, 3))], dim=1), torch.cat([torch.zeros((I.shape[0], w_3.shape[1], 3, 3)), I], dim=1)], dim=0\n",
    "    )\n",
    "    w_out_et = torch.cat(\n",
    "        [torch.cat([w_out, torch.zeros_like(I_input)], dim=1), torch.cat([torch.zeros_like(w_out), I_input], dim=1)], dim=0\n",
    "    )\n",
    "    \n",
    "    w_concat_and_skip_et = merge_sequential_conv(w_out_et, I_out)\n",
    "    \n",
    "    # body\n",
    "    x = relu(conv_3x3(x, w_proj))\n",
    "    x_cat_bef_1 = torch.cat([x, x_skip], dim=1)\n",
    "    x_cat_bef_2 = relu(conv_3x3(x_cat_bef_1, w_1_et)) # <<<<<<<<<<<<<<<<<<<<<\n",
    "    x_cat_bef_3 = relu(conv_3x3(x_cat_bef_2, w_2_et))\n",
    "    x_cat_bef = relu(conv_3x3(x_cat_bef_3, w_3_et))\n",
    "    \n",
    "    # concat + skip\n",
    "    x = conv_3x3(x_cat_bef, w_concat_and_skip_et)\n",
    "    \n",
    "    # clip\n",
    "    x = torch.clip(x, 0, 255)\n",
    "    \n",
    "    # upsample\n",
    "    x = F.pixel_shuffle(x, scale)\n",
    "    \n",
    "    return x\n",
    "\n",
    "res_8 = PlainSR_concat_noconvres_to_conv_3(x, w_proj, w_1, w_2, w_3, w_out)\n",
    "\n",
    "torch.allclose(res_1, res_8, atol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Convert Input Concat Operation into Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def PlainSR_concat_input_to_conv(x, w_proj, w_1, w_2, w_3, w_out, scale=4):\n",
    "    # x_skip = x.clone()  <<<<<<<<<<<<<<<<<<<<<\n",
    "    I = torch.eye(x.shape[1]).view(x.shape[1], x.shape[1], 1, 1)\n",
    "    I = F.pad(I, (1, 1, 1, 1))\n",
    "    I_input = torch.cat(\n",
    "        [torch.eye(x.shape[1]).view(x.shape[1], x.shape[1], 1, 1)] * scale ** 2, dim=0\n",
    "    )\n",
    "    I_input = F.pad(I_input, (1, 1, 1, 1))\n",
    "    I_out = torch.eye(w_out.shape[0]).view(w_out.shape[0], w_out.shape[0], 1, 1).repeat(1, 2, 1, 1)\n",
    "    \n",
    "    w_proj_et = torch.cat([w_proj, I], dim=0)   # <<<<<<<<<<<<<<<<<<<<<\n",
    "    w_1_et = torch.cat(\n",
    "        [torch.cat([w_1, torch.zeros((w_1.shape[0], I.shape[1], 3, 3))], dim=1), torch.cat([torch.zeros((I.shape[0], w_1.shape[1], 3, 3)), I], dim=1)], dim=0\n",
    "    )   \n",
    "    w_2_et = torch.cat(\n",
    "        [torch.cat([w_2, torch.zeros((w_2.shape[0], I.shape[1], 3, 3))], dim=1), torch.cat([torch.zeros((I.shape[0], w_2.shape[1], 3, 3)), I], dim=1)], dim=0\n",
    "    )  \n",
    "    w_3_et = torch.cat(\n",
    "        [torch.cat([w_3, torch.zeros((w_3.shape[0], I.shape[1], 3, 3))], dim=1), torch.cat([torch.zeros((I.shape[0], w_3.shape[1], 3, 3)), I], dim=1)], dim=0\n",
    "    )\n",
    "    w_out_et = torch.cat(\n",
    "        [torch.cat([w_out, torch.zeros_like(I_input)], dim=1), torch.cat([torch.zeros_like(w_out), I_input], dim=1)], dim=0\n",
    "    )\n",
    "    \n",
    "    w_concat_and_skip_et = merge_sequential_conv(w_out_et, I_out)\n",
    "    \n",
    "    # body\n",
    "    x_cat_bef_1 = relu(conv_3x3(x, w_proj_et)) # <<<<<<<<<<<<<<<<<<<<<\n",
    "    # x = relu(conv_3x3(x, w_proj))\n",
    "    # x_cat_bef_1 = torch.cat([x, x_skip], dim=1)\n",
    "    x_cat_bef_2 = relu(conv_3x3(x_cat_bef_1, w_1_et))\n",
    "    x_cat_bef_3 = relu(conv_3x3(x_cat_bef_2, w_2_et))\n",
    "    x_cat_bef = relu(conv_3x3(x_cat_bef_3, w_3_et))\n",
    "    \n",
    "    # concat + skip\n",
    "    x = conv_3x3(x_cat_bef, w_concat_and_skip_et)\n",
    "    \n",
    "    # clip\n",
    "    x = torch.clip(x, 0, 255)\n",
    "    \n",
    "    # upsample\n",
    "    x = F.pixel_shuffle(x, scale)\n",
    "    \n",
    "    return x\n",
    "\n",
    "res_8 = PlainSR_concat_input_to_conv(x, w_proj, w_1, w_2, w_3, w_out)\n",
    "\n",
    "torch.allclose(res_1, res_8, atol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Convert Clip Operation into Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def PlainSR_clip_to_conv(x, w_proj, w_1, w_2, w_3, w_out, scale=4):\n",
    "    I = torch.eye(x.shape[1]).view(x.shape[1], x.shape[1], 1, 1)\n",
    "    I = F.pad(I, (1, 1, 1, 1))\n",
    "    I_input = torch.cat(\n",
    "        [torch.eye(x.shape[1]).view(x.shape[1], x.shape[1], 1, 1)] * scale ** 2, dim=0\n",
    "    )\n",
    "    I_input = F.pad(I_input, (1, 1, 1, 1))\n",
    "    I_out = torch.eye(w_out.shape[0]).view(w_out.shape[0], w_out.shape[0], 1, 1).repeat(1, 2, 1, 1)\n",
    "    \n",
    "    w_proj_et = torch.cat([w_proj, I], dim=0)\n",
    "    w_1_et = torch.cat(\n",
    "        [torch.cat([w_1, torch.zeros((w_1.shape[0], I.shape[1], 3, 3))], dim=1), torch.cat([torch.zeros((I.shape[0], w_1.shape[1], 3, 3)), I], dim=1)], dim=0\n",
    "    )   \n",
    "    w_2_et = torch.cat(\n",
    "        [torch.cat([w_2, torch.zeros((w_2.shape[0], I.shape[1], 3, 3))], dim=1), torch.cat([torch.zeros((I.shape[0], w_2.shape[1], 3, 3)), I], dim=1)], dim=0\n",
    "    )  \n",
    "    w_3_et = torch.cat(\n",
    "        [torch.cat([w_3, torch.zeros((w_3.shape[0], I.shape[1], 3, 3))], dim=1), torch.cat([torch.zeros((I.shape[0], w_3.shape[1], 3, 3)), I], dim=1)], dim=0\n",
    "    )\n",
    "    w_out_et = torch.cat(\n",
    "        [torch.cat([w_out, torch.zeros_like(I_input)], dim=1), torch.cat([torch.zeros_like(w_out), I_input], dim=1)], dim=0\n",
    "    )\n",
    "    \n",
    "    w_concat_and_skip_et = merge_sequential_conv(w_out_et, I_out)\n",
    "    \n",
    "    # body\n",
    "    x_cat_bef_1 = relu(conv_3x3(x, w_proj_et))\n",
    "    x_cat_bef_2 = relu(conv_3x3(x_cat_bef_1, w_1_et))\n",
    "    x_cat_bef_3 = relu(conv_3x3(x_cat_bef_2, w_2_et))\n",
    "    x_cat_bef = relu(conv_3x3(x_cat_bef_3, w_3_et))\n",
    "    \n",
    "    # concat + skip\n",
    "    x = conv_3x3(x_cat_bef, w_concat_and_skip_et)\n",
    "    \n",
    "    # clip\n",
    "    negative_I_clip = -torch.eye(x.shape[1]).view(x.shape[1], x.shape[1], 1, 1)\n",
    "    y = relu(conv_1x1(x, negative_I_clip) + 255)\n",
    "    x = relu(conv_1x1(y, negative_I_clip) + 255)\n",
    "    \n",
    "    # upsample\n",
    "    x = F.pixel_shuffle(x, scale)\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "res_9 = PlainSR_clip_to_conv(x, w_proj, w_1, w_2, w_3, w_out)\n",
    "\n",
    "torch.allclose(res_1, res_9, atol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result of Converted PlainSR has no repeat, add, concat, and clip operation that slows down Mobile SoC's performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "safmn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
